---
tags:
  - llm
nanoid: mhxa3emien6s
type: fleeting_note
created_at: 2024-05-31 17:46
---

#JaiLu l'article "[[LLM]] auto-h√©berg√©s ou non : mon exp√©rience - [[LinuxFr]].org" <https://linuxfr.org/users/jobpilot/journaux/llm-auto-heberges-ou-non-mon-experience>.

> Cependant, une question cruciale se pose rapidement : faut-il les auto-h√©berger ou les utiliser via des services en ligne ? Dans cet article, je partage mon exp√©rience sur ce sujet. 

Je me suis plus ou moins pos√© cette question il y a 15 jours dans la note suivante : [[2024-05-17_1257]].

> Ces mod√®les peuvent √©galement tourner localement si vous avez un bon GPU avec suffisamment de m√©moire (32 Go, voire 16 Go pour certains mod√®les quantifi√©s sur 2 bits). Ils sont plus intelligents que les petits mod√®les, mais moins que les grands. Dans mon exp√©rience, ils suffisent dans 95% des cas pour l'aide au codage et 100% pour la traduction ou la correction de texte.

Int√©ressant comme retour d'exp√©rience.

> L'auto-h√©bergement peut se faire de mani√®re compl√®te (frontend et backend) ou hybride (frontend auto-h√©berg√© et inf√©rence sur un endpoint distant). Pour le frontend, j'utilise deux containers Docker chez moi : Chat UI de Hugging Face et Open Webui.

Je pense qu'il parle de :

- https://github.com/huggingface/chat-ui
- https://github.com/open-webui/open-webui

Je suis impressionn√© par la taille de [la liste des features de Open WebUI](https://github.co.m/open-webui/open-webui?tab=readme-ov-file#features-)

> J'ai achet√© d'occasion un ordinateur Dell Precision 5820 avec 32 Go de RAM, un CPU Xeon W-2125, une alimentation de 900W et deux cartes NVIDIA Quadro P5000 de 16 Go de RAM chacune, pour un total de 646 CHF. 

#JeMeDemande comment se situe la carte graphique [NVIDIA Quadro P5000](https://en.wikipedia.org/wiki/Quadro) sur le march√© ü§î.

> J'ai install√© Ubuntu Server 22.4 avec Docker et les pilotes NVIDIA. Ma machine dispose donc de 32 Go de RAM GPU utilisables pour l'inf√©rence. J'utilise OLLaMa, r√©parti sur les deux cartes, et Mistral 8x7b quantifi√© sur 4 bits (2 bits sur une seule carte, mais l'inf√©rence est deux fois plus lente). En inf√©rence, je fais environ **24 tokens/seconde**. Le chargement initial du mod√®le (24 Go) prend un peu de temps. J'ai √©galement essay√© LLaMA 3 70b quantifi√© sur 2 bits, mais c'est tr√®s lent **(3 tokens/seconde)**.

Benchmark int√©ressant.

> En inf√©rence, la consommation monte √† environ 420W, soit une puissance suppl√©mentaire de 200W. Sur 24h, cela repr√©sente une consommation de 6,19 kWh, soit un co√ªt de 1,61 CHF/jour.

Soit environ 1,63 ‚Ç¨ par jour.

> Together AI est une soci√©t√© am√©ricaine qui offre un cr√©dit de 25$ √† l'ouverture d'un compte. Les prix sont les suivants :
> - Mistral 8x7b : 0,60$/million de tokens
> - LLaMA 3 70b : 0,90$/million de tokens
> - Mistral 8x22b : 1,20$/million de tokens

#JaiD√©couvert https://www.together.ai/pricing

Comparaison avec les prix de OpenIA API :

![[Pasted image 20240531180556.png]]

![[Pasted image 20240531180620.png]]

#JeMeDemande si l'unit√© tokens est comparable entre les mod√®les ü§î.


