---
tags:
  - llm
  - MachineLearning
nanoid: 68lbuczv8qol
type: fleeting_note
---
Cette semaine, j'ai d√©jeun√© avec un ami dont les connaissances dans le domaine du #MachineLearning et des #LLM d√©passent largement les miennes... J'en ai profit√© pour lui poser de nombreuses questions.  
Voici ci-dessous quelques notes de ce que j'ai retenu de notre discussion.

**Avertissement :** Le contenu de cette note refl√®te les informations que j'ai re√ßues pendant cette conversation. Je n'ai pas v√©rifi√© l'exactitude de ces informations, et elles pourraient ne pas √™tre enti√®rement correctes. Le contenu de cette note est donc √† consid√©rer comme approximatif. N'h√©sitez pas √† me contacter √† <contact@stephane-klein.info> si vous constatez des erreurs.

## Histoire de Llama.cpp ?

**Question : quelle est l'histoire de [[Llama.cpp]] ? Comment ce projet se positionne dans l'√©cosyst√®me ?**

D'apr√®s ce que j'ai compris, d√©but 2023, [[PyTorch]] √©tait la solution "mainstream" (la seule ?) pour [[Inference Engines|effectuer de l'inf√©rence]] sur le mod√®le [[LLaMa]] ‚Äî sortie en f√©vrier 2023.  

PyTorch ‚Äî √©crit en Python et C++ ‚Äî est optimis√©e pour les GPU, plus pr√©cis√©ment pour le framework [[CUDA]].  
PyTorch est n'est pas optimis√© pour l'ex√©cution sur CPU, ce n'est pas son objectif.

[[Georgi Gerganov]] a cr√©√© [[Llama.cpp]] pour pouvoir [[Inference Engines|effectuer de l'inf√©rence]] sur le mod√®le [[LLaMa]] sur du CPU d'une mani√®re optimis√©. Contrairement √† [[PyTorch]], plus de Python et des optimisations pour Apple Silicon, utilisation des instructions [[Advanced Vector Extensions|AVX / AVX2]] sur les CPU x86‚Ä¶
Par la suite, ¬´ la boucle a √©t√© boucl√©e ¬ª avec l'ajout du support GPU en avril 2023.

√Ä la question ¬´ Maintenant que [[Llama.cpp]] a un support GPU, √† quoi sert [[PyTorch]] ? ¬ª, la r√©ponse est : [[PyTorch]] permet beaucoup d'autres choses, comme entra√Æner des mod√®les‚Ä¶

Aper√ßu de l'historique du projet :

- 18 septembre 2022 : [[Georgi Gerganov]] commence la librairie [[ggml]], sur laquelle seront construits [[Llama.cpp]] et [[Whisper.cpp]].
- 4 mars 2023 : [[Georgi Gerganov]] a publi√© le premier commit de [llama.cpp](https://github.com/ggerganov/llama.cpp/graphs/contributors).
- 10 mars 2023 : je crois que c'est le premier poste Twitter de publication de [[Llama.cpp]] <https://twitter.com/ggerganov/status/1634282694208114690>.
- 13 mars 2023 : premier post √† propos de LLama.cpp sur Hacker News qui fait z√©ro commentaire - [Llama.cpp can run on Macs that have 64G of RAM (40GB of Free memory)](https://news.ycombinator.com/item?id=35135375).
- 14 mars 2023 : second poste, toujours z√©ro commentaire - [Run a GPT-3 style AI on your local machine, fully on premise](https://news.ycombinator.com/item?id=35146267).
- 31 mars 2023 : premier thread sur [[Llama.cpp]] qui fait le *buzz* avec 414 commentaires - [Llama.cpp 30B runs with only 6GB of RAM now](https://news.ycombinator.com/item?id=35393284).
- 12 avril 2023 : d'apr√®s ce que je comprends, voici la Merge Request d'ajout du support [[GPU]] √† [[Llama.cpp]] [# Add GPU support to ggml](https://github.com/ggerganov/llama.cpp/discussions/915) ([from](https://twitter.com/ggerganov/status/1646162608133750787)).
- 6 juin 2023 : [[Georgi Gerganov]] lance sa soci√©t√© nomm√©e https://ggml.ai ([from](https://twitter.com/ggerganov/status/1666120568993730561)) .
- 10 juillet 2023 : [Distributed inference via MPI](https://github.com/ggerganov/llama.cpp/pull/2099) - Model inference is currently limited by the memory on a single node. Using MPI, we can distribute models across a locally networked cluster of machines.
- 24¬†juillet 2023 : [llama : add support for llama2.c models](https://github.com/ggerganov/llama.cpp/issues/2379) ([from](https://twitter.com/ggerganov/status/1683574709470875649)).
- 25 ao√ªt 2023 : [ajout du support ROCm](https://github.com/ggerganov/llama.cpp/pull/1087) ([[AMD]]).

## Comment nommer Llama.cpp ?

**Question : quel est le nom d'un outil comme [[Llama.cpp]] ?**

R√©ponse : Je n'ai pas eu de r√©ponse univoque √† cette question.

C'est un outil qui effectue des inf√©rences sur un mod√®le.

Voici quelques id√©es de nom :

- Moteur d'inf√©rence ([[Inference Engines]]) ;
- Ex√©cuteur d'inf√©rence (Inference runtime) ;
- Biblioth√®que d'inf√©rence.

Personnellement, #JaiD√©cid√©  d'utiliser le terme [[Inference Engines]].

## Autre projet comme Llama.cpp ?

**Question : Existe-t-il un autre projet comme Llama.cpp**

Oui, il existe d'autres projets, comme [llm](https://github.com/rustformers/llm) - Large Language Models for Everyone, in Rust. Article Hacker News publi√© le 14 mars 2023 sous le nom [LLaMA-rs: a Rust port of llama.cpp for fast LLaMA inference on CPU](https://news.ycombinator.com/item?id=35151173).

Et aussi, https://github.com/karpathy/llm.c - LLM training in simple, raw C/CUDA ([from](https://twitter.com/karpathy/status/1777427944971083809)).  
Le README de ce projet [liste de nombreuses autres impl√©mentations](https://github.com/karpathy/llm.c?tab=readme-ov-file#notable-forks) de [[Inference Engines]].

Mais, √† ce jour, [[Llama.cpp]] semble √™tre l'[[Inference Engines]] le plus complet et celui qui fait consensus.

## GPU vs CPU

**Question : Jai l'impression qu'il est possible de compiler des programmes g√©n√©ralistes sur GPU, dans ce cas, pourquoi ne pas remplacer les CPU par des GPU ? Pourquoi ne pas tout ex√©cuter par des GPU ?**

Mon ami n'a pas eu une r√©ponse non √©quivoque √† cette question. Il m'a r√©pondu que l'int√©r√™t du CPU reste sans doute sa faible consommation √©nergique par rapport au GPU.

Apr√®s ce d√©jeuner, j'ai fait des recherches et je suis tomb√© sur l'article Wikipedia nomm√© [General-purpose computing on graphics processing units](https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units) ([je suis tomb√© dessus via l'article ROCm](https://en.wikipedia.org/wiki/ROCm)).

Cet article contient une section nomm√©e [GPU vs. CPU](https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units#GPU_vs._CPU), mais qui ne r√©pond pas √† mes questions √† ce sujet ü§∑‚Äç‚ôÇÔ∏è.

## ROCm ?

**Question : J'ai du mal √† comprendre [[ROCm]], j'ai l'impression que cela apporte le support du framework CUDA sur [[AMD]], c'est bien cela ?**

R√©ponse : oui.

J'ai ensuite lu [ici](https://en.wikipedia.org/wiki/ROCm#HIPIFY) :

> HIPIFY is a source-to-source compiling tool. It translates CUDA to HIP and reverse, either using a Clang-based tool, or a sed-like Perl script. 

## RAG ?

**Question : comment setup facilement un [[RAG]] ?**

R√©ponse : regarde [[llama_index]].

#JaiD√©couvert ensuite https://github.com/abetlen/llama-cpp-python

> Simple Python bindings for @ggerganov's llama.cpp library. This package provides:
> - Low-level access to C API via ctypes interface.
> - High-level Python API for text completion
> 	- OpenAI-like API
> 	- LangChain compatibility
> 	- **LlamaIndex compatibility**
> - ...

## dottextai / outlines

Il m'a partag√© le projet https://github.com/outlines-dev/outlines alias [dottxtai](https://twitter.com/dottxtai), pour le moment, je ne sais pas trop √† quoi √ßa sert, mais je pense que c'est int√©ressant.

## Embedding ?

**Question : [[Thibault Neveu]] parle souvent d'[[Word embedding|embedding]] dans ses vid√©os et j'ai du mal √† comprendre concr√®tement ce que c'est, tu peux m'expliquer ?**

Le vrai terme est [Word embedding](https://en.wikipedia.org/wiki/Word_embedding) et d'apr√®s ce que j'ai compris, en simplifiant, je dirais que c'est le r√©sultat d'une "[s√©rialisation](https://fr.wikipedia.org/wiki/S%C3%A9rialisation)" de mots ou de textes.

#JaiD√©couvert ensuite l'article [Word Embeddings in NLP: An Introduction](https://hunterheidenreich.com/posts/intro-to-word-embeddings/) ([from](https://www.scaleway.com/en/blog/building-a-machine-reading-comprehension-system-using-the-latest-advances-in-deep-learning-for-nlp/)) que j'ai survol√©. #JaimeraisUnJour prendre le temps de le lire avec attention.

## Transformers ?

**Question : et maintenant, peux-tu me vulgariser le concept de [[Transformer (deep learning architecture)|transformer]] ?**

R√©ponse : non, je t'invite √† lire l'article [Natural Language Processing: the age of Transformers](https://www.scaleway.com/en/blog/building-a-machine-reading-comprehension-system-using-the-latest-advances-in-deep-learning-for-nlp/).

## Entrainement d√©centralis√© ?

**Question : existe-t-il un syst√®me communautaire pour permettre de g√©n√©rer des mod√®les de mani√®re d√©centralis√©e ?**

R√©ponse - Oui, voici quelques liens :

- [BigScience Research Workshop](https://bigscience.huggingface.co)/
- [Distributed Deep Learning in Open Collaborations](https://arxiv.org/abs/2106.10207)
- [Deep Learning over the Internet: Training Language Models Collaboratively](https://huggingface.co/blog/collaborative-training)

Au passage, j'ai ajout√© https://huggingface.co/blog/ √† mon agr√©gateur RSS ([miniflux](https://github.com/miniflux/v2)).

## La suite‚Ä¶

Nous avons parl√© de nombreux autres sujets sur cette th√©matique, mais j'ai d√©cid√© de m'arr√™ter l√† pour cette note et de la publier. Peut-√™tre que je publierai la suite un autre jour ü§∑‚Äç‚ôÇÔ∏è.
