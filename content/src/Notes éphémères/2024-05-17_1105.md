---
tags:
  - RAG
  - llm
  - MachineLearning
nanoid: fzupxrs4abda
---
Dans l'article "[Qu'est-ce que la g√©n√©ration augment√©e de r√©cup√©ration (RAG, retrieval-augmented generation) ?](https://www.oracle.com/fr/artificial-intelligence/generative-ai/retrieval-augmented-generation-rag/)" je d√©couvre l'acronyme [[RAG|G√©n√©ration Augment√©e de R√©cup√©ration]].

Je constate qu'il  existe [un paragraphe](https://en.wikipedia.org/wiki/Prompt_engineering#Retrieval-augmented_generation) √† ce sujet sur Wikipedia.

> The initial phase utilizes dense embeddings to retrieve documents. ([from](https://en.wikipedia.org/wiki/Prompt_engineering#Retrieval-augmented_generation))

Je tombe encore une fois sur "[[embeddings]]", #JaimeraisUnJour prendre le temps de comprendre correctement cette notion.

> Prenez l'exemple d'une ligue sportive qui souhaite que les fans et les m√©dias puisse utiliser un chat pour acc√©der √† ses donn√©es et obtenir des r√©ponses √† leurs questions sur les joueurs, les √©quipes, l'histoire et les r√®gles du sport, ainsi que les statistiques et les classements actuels. Un LLM g√©n√©ralis√© pourrait r√©pondre √† des questions sur l'histoire et les r√®gles ou peut-√™tre d√©crire le stade d'une √©quipe donn√©e. Il ne serait pas en mesure de discuter du jeu de la nuit derni√®re ou de fournir des informations actuelles sur la blessure d'un athl√®te, parce que le LLM n'aurait pas ces informations. √âtant donn√© qu'un LLM a besoin d'une puissance de calcul importante pour se r√©entra√Æner, il n'est pas possible de maintenir le mod√®le √† jour. ([from](https://www.oracle.com/fr/artificial-intelligence/generative-ai/retrieval-augmented-generation-rag/)]).

Le contenu de ce paragraphe m'int√©resse beaucoup, parce qu'en 2023, c'√©tait l'objectif que j'avais en cr√©ant l'issue <https://github.com/stephane-klein/backlog/issues/226>.

Sans avoir fait de recherche, je pensais que la seule solution pour faire apprendre de nouvelles choses ‚Äî injecter de nouvelle donn√©es ‚Äî dans un mod√®le √©tait de faire du [[fine-tuning]].

En lisant ce paragraphe, je pense comprendre que le [[Fine-tuning|fine-tuning]] n'est pas la seule solution, ni m√™me, j'ai l'impression, la "bonne" solution pour le *use-case* que j'aimerais mettre en pratique. 

> En plus du LLM assez statique, la ligue sportive poss√®de ou peut acc√©der √† de nombreuses autres sources d'information, y compris les bases de donn√©es, les entrep√¥ts de donn√©es, les documents contenant les biographies des joueurs et les flux d'actualit√©s d√©taill√©es concernant chaque jeu. ([from](https://www.oracle.com/fr/artificial-intelligence/generative-ai/retrieval-augmented-generation-rag/)])

#JaimeraisUnJour impl√©menter un POC pour mettre cela en pratique.

> Dans la RAG, cette grande quantit√© de donn√©es dynamiques est convertie dans un format commun et stock√©e dans une biblioth√®que de connaissances accessible au syst√®me d'IA g√©n√©rative.
>
> Les donn√©es de cette biblioth√®que de connaissances sont ensuite trait√©es en repr√©sentations num√©riques √† l'aide d'un type sp√©cial d'algorithme appel√© mod√®le de langage int√©gr√© et stock√©es dans une base de donn√©es vectorielle, qui peut √™tre rapidement recherch√©e et utilis√©e pour r√©cup√©rer les informations contextuelles correctes.

Int√©ressant.

> Il est int√©ressant de noter que si le processus de formation du LLM g√©n√©ralis√© est long et co√ªteux, c'est tout √† fait l'inverse pour les mises √† jour du mod√®le RAG. De nouvelles donn√©es peuvent √™tre charg√©es dans le mod√®le de langage int√©gr√© et traduites en vecteurs de mani√®re continue et incr√©mentielle. Les r√©ponses de l'ensemble du syst√®me d'IA g√©n√©rative peuvent √™tre renvoy√©es dans le mod√®le RAG, am√©liorant ses performances et sa pr√©cision, car il sait comment il a d√©j√† r√©pondu √† une question similaire.

Ok, si je comprends bien, c'est la "kill feature" du [[RAG]] versus du [[Fine-tuning|fine-tuning]].

> bien que la mise en oeuvre de l'IA g√©n√©rative avec la RAG est plus co√ªteux que l'utilisation d'un LLM seul, il s'agit d'un meilleur investissement √† long terme en raison du r√©entrainement fr√©quent du LLM

Ok.

Bilan de cette lecture, je dis merci √† [[Alexandre]] de me l'avoir partag√©, j'ai appris [[RAG]] et #JePense que c'est une technologie qui me sera tr√®s utile √† l'avenir üëå.
