---
tags:
  - Inference
  - MachineLearning
  - vocabulaire
  - llm
nanoid: 6o2chokdnjnx
type: journal_note
created_at: 2024-06-08 10:35
---
Dans [[2024-06-06_1047]] #JaiDÃ©cidÃ© d'utiliser le terme [[Inference Engines]] pour dÃ©finir la fonction ou la catÃ©gorie de [[Llama.cpp]].

J'ai Ã©changÃ© avec un ami au sujet des [[NPU]] et j'ai dit que j'avais l'impression que ces puces sont spÃ©cialÃ©s pour exÃ©cuter des [[Inference Engines]], c'est-Ã -dire, effectuer des calculs d'infÃ©rence Ã  partir de modÃ¨les.

AprÃ¨s vÃ©rification, dans [cet article](https://en.wikipedia.org/wiki/AI_accelerator) je lis :

> An AI accelerator, deep learning processor, or **neural processing unit (NPU)** is a class of specialized hardware accelerator or computer system designed to **accelerate artificial intelligence** and **machine learning** applications, including artificial neural networks and machine vision. 

et je comprends que mon impression Ã©tait fausse. Il semble que les NPU ne sont pas seulement dÃ©diÃ©s aux opÃ©rations d'exÃ©cution d'infÃ©rence, mais semblent Ãªtre optimisÃ©s aussi pour faire de l'entrainement ðŸ¤”.

Un ami me prÃ©cise :

> > Inference Engines
>
> Pour moi, c'est un terme trÃ¨s gÃ©nÃ©rique qui couvre tous les aspects du machine learning, du deep learning et des algorithmes type LLM mis en Å“uvre.

et il me partage [l'article Wikipedia Inference engine](https://en.wikipedia.org/wiki/Inference_engine) que je n'avais pas lu quand j'avais rÃ©digÃ© [[2024-06-06_1047]], honte Ã  moi ðŸ«£.

Dans [l'article Wikipedia Inference engine](https://en.wikipedia.org/wiki/Inference_engine) je lis :

> In the field of artificial intelligence, an inference engine is a software component of an intelligent system that applies logical rules to the knowledge base to deduce new information.

et 

> Additionally, the concept of 'inference' has expanded to include the process through which trained neural networks generate predictions or decisions. In this context, an 'inference engine' could refer to the specific part of the system, or even the hardware, that executes these operations.

Je comprends qu'un [[Inference Engines]] n'effectue **pas** l'entrainement de modÃ¨les.

Pour Ã©viter la confusion, #JaiDÃ©cidÃ© d'utiliser Ã  l'avenir le terme "[[Inference Engines|Inference Engine (comme LLama.cpp)]]".

Le contenu de [l'article Wikipedia Llama.cpp](https://en.wikipedia.org/wiki/Llama.cpp) augmente mon niveau de confiance dans ce choix de vocabulaire :

> llama.cpp is an open source software library written in C++, **that performs inference** on various Large Language Models such as Llama

