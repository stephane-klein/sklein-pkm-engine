---
tags:
  - llm
  - MachineLearning
nanoid: m3hgk78zz62o
type: fleeting_note
created_at: 2024-06-06 16:20
---
En travaillant sur [[2024-06-06_1047]] :

- #JaiDÃ©couvert https://github.com/PABannier/bark.cpp - Suno AI's Bark model in C/C++ for fast text-to-speech ([from](https://twitter.com/el_PA_B/status/1782372867872547063))
- #JaiDÃ©couvert https://github.com/karpathy/llm.c - LLM training in simple, raw C/CUDA ([from](https://twitter.com/karpathy/status/1777427944971083809))
- #JaiLu au sujet de [[GGUF]] :

> Hugging Face Hub supports all file formats, but has built-in features for [GGUF format](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md), a binary format that is optimized for quick loading and saving of models, making it highly efficient for inference purposes. GGUF is designed for use with GGML and other executors. GGUF was developed by [@ggerganov](https://huggingface.co/ggerganov) who is also the developer of [llama.cpp](https://github.com/ggerganov/llama.cpp), a popular C/C++ LLM inference framework.

https://huggingface.co/docs/hub/gguf

- #JaiDÃ©couvert [llama : add pipeline parallelism support by slaren](https://github.com/ggerganov/llama.cpp/pull/6017) autrement dit Â« Multi-GPU pipeline parallelism support Â» ([from](https://twitter.com/ggerganov/status/1768357343060689061))
- #JaiDÃ©couvert https://github.com/ggerganov/whisper.cpp de [[Georgi Gerganov]]
- #JaiDÃ©couvert https://github.com/ggerganov/llama.cpp/discussions/3471
- #JaiDÃ©couvert la Merge Request d'ajout du support de ROCm Port : [ROCm Port 1087](https://github.com/ggerganov/llama.cpp/pull/1087) ([from](https://twitter.com/ggerganov/status/1695170653547016617))
- #JaiDÃ©couvert [Basic Vim plugin for llama.cpp](https://twitter.com/ggerganov/status/1688878489015844864)
- #JaiDÃ©couvert https://github.com/rgerganov/ggtag par le mÃªme auteur que [[Llama.cpp]], c'est-Ã -dire [[Georgi Gerganov]]
- #JaiDÃ©couvert  [Distributed inference via MPI](https://github.com/ggerganov/llama.cpp/pull/2099) - Model inference is currently limited by the memory on a single node. Using MPI, we can distribute models across a locally networked cluster of machines.
- #JaiDÃ©couvert : d'aprÃ¨s ce que j'ai compris la librairie [[ggml]] est le composant de base de [[Llama.cpp]] et [[Whisper.cpp]]
- #JaiDÃ©couvert que [[Georgi Gerganov]] a lancÃ© sa sociÃ©tÃ© nommÃ©e https://ggml.ai ([from](https://twitter.com/ggerganov/status/1666120568993730561)) et que celle-ci est financÃ© entre autre part [[Nat Friedman]] ! Ha ha, encore lui ðŸ˜.

> ggml.ai is a company founded by Georgi Gerganov to support the development of ggml. Nat Friedman and Daniel Gross provided the pre-seed funding.
> 
> We are currently seeking to hire full-time developers that share our vision and would like to help advance the idea of on-device inference. If you are interested and if you have already been a contributor to any of the related projects, please contact us at jobs@ggml.ai 

- #JaiDÃ©couvert Text-to-phoneme-to-speech https://twitter.com/ConcreteSciFi/status/1641166275446714368, j'adore ðŸ™‚
