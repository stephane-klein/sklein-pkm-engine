---
tags:
  - llm
  - MachineLearning
nanoid: m3hgk78zz62o
type: fleeting_note
created_at: 2024-06-06 16:20
---
En travaillant sur [[2024-06-06_1047]] :

- #JaiDécouvert https://github.com/PABannier/bark.cpp - Suno AI's Bark model in C/C++ for fast text-to-speech ([from](https://twitter.com/el_PA_B/status/1782372867872547063))
- #JaiDécouvert https://github.com/karpathy/llm.c - LLM training in simple, raw C/CUDA ([from](https://twitter.com/karpathy/status/1777427944971083809))
- #JaiLu au sujet de [[GGUF]] :

> Hugging Face Hub supports all file formats, but has built-in features for [GGUF format](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md), a binary format that is optimized for quick loading and saving of models, making it highly efficient for inference purposes. GGUF is designed for use with GGML and other executors. GGUF was developed by [@ggerganov](https://huggingface.co/ggerganov) who is also the developer of [llama.cpp](https://github.com/ggerganov/llama.cpp), a popular C/C++ LLM inference framework.

https://huggingface.co/docs/hub/gguf

- #JaiDécouvert [llama : add pipeline parallelism support by slaren](https://github.com/ggerganov/llama.cpp/pull/6017) autrement dit « Multi-GPU pipeline parallelism support » ([from](https://twitter.com/ggerganov/status/1768357343060689061))
- #JaiDécouvert https://github.com/ggerganov/whisper.cpp de [[Georgi Gerganov]]
- #JaiDécouvert https://github.com/ggerganov/llama.cpp/discussions/3471
- #JaiDécouvert la Merge Request d'ajout du support de ROCm Port : [ROCm Port 1087](https://github.com/ggerganov/llama.cpp/pull/1087) ([from](https://twitter.com/ggerganov/status/1695170653547016617))
- #JaiDécouvert [Basic Vim plugin for llama.cpp](https://twitter.com/ggerganov/status/1688878489015844864)
- #JaiDécouvert https://github.com/rgerganov/ggtag par le même auteur que [[Llama.cpp]], c'est-à-dire [[Georgi Gerganov]]
- #JaiDécouvert  [Distributed inference via MPI](https://github.com/ggerganov/llama.cpp/pull/2099) - Model inference is currently limited by the memory on a single node. Using MPI, we can distribute models across a locally networked cluster of machines.
- #JaiDécouvert : d'après ce que j'ai compris la librairie [[ggml]] est le composant de base de [[Llama.cpp]] et [[Whisper.cpp]]
- #JaiDécouvert que [[Georgi Gerganov]] a lancé sa société nommée https://ggml.ai ([from](https://twitter.com/ggerganov/status/1666120568993730561)) et que celle-ci est financé entre autre part [[Nat Friedman]] ! Ha ha, encore lui 😍.

> ggml.ai is a company founded by Georgi Gerganov to support the development of ggml. Nat Friedman and Daniel Gross provided the pre-seed funding.
> 
> We are currently seeking to hire full-time developers that share our vision and would like to help advance the idea of on-device inference. If you are interested and if you have already been a contributor to any of the related projects, please contact us at jobs@ggml.ai 

- #JaiDécouvert Text-to-phoneme-to-speech https://twitter.com/ConcreteSciFi/status/1641166275446714368, j'adore 🙂
